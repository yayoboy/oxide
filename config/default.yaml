# Oxide Configuration File
# Configure LLM services and routing rules

services:
  # Gemini CLI - Large context window ideal for codebase analysis
  gemini:
    type: cli
    executable: gemini
    enabled: true
    max_context_tokens: 2000000  # 2M tokens
    capabilities:
      - codebase_analysis
      - architecture_design
      - multi_file_context

  # Qwen CLI - Code-specialized for review and generation
  qwen:
    type: cli
    executable: qwen
    enabled: true
    max_context_tokens: 32000
    capabilities:
      - code_review
      - code_generation
      - debugging
      - refactoring

  # Ollama Local - Fast local inference with auto-start
  ollama_local:
    type: http
    base_url: "http://localhost:11434"
    api_type: ollama
    enabled: true
    # Model configuration (can be null for auto-detection)
    default_model: "qwen2.5-coder:7b"
    models:
      - "qwen2.5-coder:7b"
      - "qwen2.5-coder:14b"
      - "qwen3-coder:latest"
    # Enhanced features
    auto_start: true              # Auto-start Ollama if not running
    auto_detect_model: true       # Auto-detect best model if default_model fails
    max_retries: 2                # Retry attempts on failure
    retry_delay: 2                # Seconds between retries
    capabilities:
      - quick_query
      - code_generation
      - documentation

  # Ollama Remote - Server on LAN for distributed processing
  ollama_remote:
    type: http
    base_url: "http://192.168.1.46:11434"
    api_type: ollama
    enabled: false  # Enable manually after setup
    default_model: null           # Will auto-detect
    auto_start: false             # Can't auto-start remote
    auto_detect_model: true       # Auto-detect available models
    max_retries: 2
    retry_delay: 2
    capabilities:
      - parallel_processing
      - code_generation

  # LM Studio - OpenAI-compatible API on laptop
  lmstudio:
    type: http
    base_url: "http://192.168.1.33:1234/v1"
    api_type: openai_compatible
    enabled: false  # Enable manually after setup (change to true when ready)
    default_model: null           # Will auto-detect via /v1/models
    auto_start: false             # LM Studio must be started manually
    auto_detect_model: true       # CRITICAL: Auto-detect model (LM Studio doesn't persist model names)
    max_retries: 2
    retry_delay: 2
    # Preferred models to look for (in priority order)
    preferred_models:
      - "qwen"
      - "coder"
      - "codellama"
      - "deepseek"

# Routing Rules - Define how tasks are routed to services
routing_rules:
  # Large codebase analysis
  codebase_analysis:
    primary: gemini
    fallback:
      - qwen
      - ollama_local
    parallel_threshold_files: 20  # Use parallel if >20 files
    timeout_seconds: 180

  # Code review tasks
  code_review:
    primary: qwen
    fallback:
      - ollama_local
      - ollama_remote
    timeout_seconds: 60

  # Code generation tasks
  code_generation:
    primary: qwen
    fallback:
      - ollama_local
    timeout_seconds: 45

  # Quick queries
  quick_query:
    primary: ollama_local
    fallback:
      - ollama_remote
      - lmstudio
    timeout_seconds: 10

  # Architecture design
  architecture_design:
    primary: gemini
    fallback:
      - qwen
    timeout_seconds: 120

  # Debugging tasks
  debugging:
    primary: qwen
    fallback:
      - ollama_local
      - gemini
    timeout_seconds: 60

  # Documentation writing
  documentation:
    primary: ollama_local
    fallback:
      - qwen
      - gemini
    timeout_seconds: 30

  # Refactoring tasks
  refactoring:
    primary: qwen
    fallback:
      - ollama_local
      - ollama_remote
    timeout_seconds: 60

# Execution Settings
execution:
  max_parallel_workers: 4
  timeout_seconds: 120
  streaming: true
  retry_on_failure: true
  max_retries: 5

# Logging Configuration
logging:
  level: INFO
  file: /tmp/oxide.log
  console: true

# Context Memory Configuration
memory:
  enabled: true
  storage_path: ~/.oxide/memory.json
  max_conversations: 1000          # Maximum number of conversations to store
  max_age_days: 30                 # Auto-prune conversations older than this
  max_messages_per_conversation: 100  # Limit messages per conversation
  auto_prune_enabled: true         # Enable automatic pruning
  similarity_threshold: 0.5        # Minimum similarity for context retrieval
